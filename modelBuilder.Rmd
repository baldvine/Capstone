---
title: "Data Science Capstone - Model Builder"
author: "Baldvin Einarsson"
date: "11/8/2017"
output:
  html_document:
    keep_md: yes
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(upload.fun = image_uri)
opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/cache/modelBuilder/")

library(ggplot2)
library(magrittr)
library(tokenizers)
library(tm)
library(RWeka)

# Holy smokes, it took forever to install rJava, see
# https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)

# library(ctv)
# install.views("NaturalLanguageProcessing")
```


# Load Data

The data comes from the following location:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), which we've downloaded.

Now, in order to load into `R`, let's make our lives easier and create an object called `fileNames.US` which contains the file names. It's value is the following:
```{r setFileNamesEtc, echo=FALSE}
# Set the directory ...
fileDir <- "./data/final/"
# ... and language file name conventions:
fileLanguage <- list(English = "en_US", Finnish = "fi_FI",
                     German = "de_DE", Russian = "ru_RU")

# This gets appended to the `fileDir.lang`
fileNameSuffix <- list(Blogs = '.blogs.txt',
                       News = '.news.txt',
                       Twitter = '.twitter.txt')
# Set up a list of file names:
fileNames.US <- 
    as.list(paste0(fileDir,fileLanguage$English, "/", 
                   fileLanguage$English,fileNameSuffix))
names(fileNames.US) <- names(fileNameSuffix)
paste(names(fileNames.US), unlist(fileNames.US), sep = ": ", collapse = "\n") %>% cat
```

We are now in a position to read in the files. Note that by typing `file` in a terminal, followed by the file name, the file encoding was listed as UTF-8.
```{r readData, message=FALSE, results='hide', cache=TRUE}
fileConnections <- lapply(X = fileNames.US, FUN = file)
corpus <- lapply(X = fileConnections, FUN = readLines, skipNul = TRUE, encoding = "UTF-8")
lapply(X = fileConnections, FUN = close)
```

# Exploratory Data Analysis

Let's do some basic inspection of the files and loaded objects. This is pretty basic, so we hide the code. Note that the word count comes from package `tokenizers`:
```{r basicFileAndObjectInfo, echo=FALSE, cache=TRUE, dependson="readData"}
kable(data.frame(Type = names(fileNames.US),
                 FileName = unlist(fileNames.US),
                 FileSize = (fileNames.US %>% 
                                 sapply(., FUN = file.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 ObjectSize = (corpus %>% 
                                 sapply(., FUN = object.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 LineCount = format(sapply(corpus, length),
                                    big.mark = ",", nsmall = 0, digits = 4),
                 WordCount =  sapply(X = corpus, 
                                     FUN = function(x){
                                         tokenize_words(x, lowercase = TRUE) %>%
                                             unlist %>%
                                             unique %>%
                                             length %>%
                                             format(big.mark = ",", digits = 4, nsmall = 0)
                                         })
                 ),
      row.names = FALSE,
      col.names = c("File type", "File name","File Size (MB)", "Object Size (MB)", 
                    "Line count", "Word (from 'tokenizers')"), 
      align = c("l","l", "r","r","r","r")
      )
```

We want to clean the data bit, such as

* change to lower case
* remove Internet links
* remove non-ascii characters
* remove double entries
* remove superfluous whitespace
* remove punctuation
* remove numbers

In order to do this, let's have a look at the [Natural Langue Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). There are plenty of packages to choose from, but for now let's start with [tm](https://cran.r-project.org/web/packages/tm/index.html).

In order to get things done faster, we pick out 1 % of entries from each type of data sources:
```{r corpusSubsetting, cache=TRUE, dependson="readData"}
set.seed(42)
useRatio <- 0.01
subsetCorpus <- 
    c(sample(corpus$Blogs, size = useRatio*length(corpus$Blogs), replace = FALSE),
      sample(corpus$News, size = useRatio*length(corpus$News), replace = FALSE),
      sample(corpus$Twitter, size = useRatio*length(corpus$Twitter), replace = FALSE)
    )
rm(corpus); invisible(gc())
```


We then follow the instructions in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) and create an object which `tm` can work with, followed by a lot of cleaning. Most are steps are self-explanatory, but comments are provided:
```{r corpusCleaning, echo=TRUE, cache=TRUE, dependson=c("corpusSubsetting","readData")}
# Here, VCorpus stands for "Volatile Corpus"
# Also, using VectorSource, means that each vector entry is treated like a "document"
myCorpus <- VCorpus(VectorSource(subsetCorpus))

# Function to remove patterns
toRemove <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
toRemove.empty <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toChange <- content_transformer(function(x, pattern, replacement) gsub(pattern, replacement, x))
toASCII <- content_transformer(function(x) iconv(x, "", "ASCII", "byte"))
# OK, so here we go:

# Set to ASCII:
myCorpus <- tm_map(myCorpus, toASCII)
# Remove troublesome non-ASCII:
myCorpus <- tm_map(myCorpus, toRemove.empty, "<.{2}>")
# Remove ftp or http links:
myCorpus <- tm_map(myCorpus, toRemove, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# Remove twitter handles (i.e. anything which has @ symbol followed by non-blank spaces:
myCorpus <- tm_map(myCorpus, toRemove, "@[^\\s]+")
# Set to lower case
myCorpus <- tm_map(myCorpus, tolower)
# Remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# Remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# Strip of superfluous whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# Set to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
```

Finally, let's remove the so called "stop words", but save into a new object called `myCorpus.noStop`. This object we will save for the trigrams and higher n-grams, because phrases of that length probably contain a stop word.:
```{r}
# Remove so called "stop words". Type stopwords("en") to see which they are
myCorpus.noStop <- tm_map(myCorpus, removeWords, stopwords("en"))
```
Let's inspect which words we just removed:
```{r showStopWords}
stopwords("en")
```

```{r echo=FALSE}
invisible(gc())
```
Note that we have not done any word stemming so far, which might reduce memory usage. However, this is something I'll have to investigate further.

Again, we follow the examples in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf), and tokenize the data, and create n-grams for values 1, 2, and 3. 

```{r ngrams, cache=TRUE, dependson=c("corpusCleaning","corpusSubsetting", "readData")}
unigramTokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
BigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
TrigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 3, max = 3))
}

unigrams <- 
    DocumentTermMatrix(myCorpus.noStop, 
                       control = list(tokenize = unigramTokenizer))
bigrams <- DocumentTermMatrix(myCorpus.noStop, control = list(tokenize = BigramTokenizer))

trigrams <- DocumentTermMatrix(myCorpus, control = list(tokenize = TrigramTokenizer))
```
```{r echo=FALSE}
invisible(gc())
```

Then, we look at he n-grams with the highest frequency (found manually), an plot histograms. Again, we show the code for 1-grams, but hide the rest, which is nearly identical:

```{r}
getUnigramsFreq <- function(termNum) {
    sum(unigrams[,termNum])
}

tmp <- data.frame(Term = unigrams$dimnames$Terms[1:5000],
                  Frequency = sapply(X = 1:5000,#dim(unigrams)[2], 
                                     FUN = getUnigramsFreq)
                  )

```


```{r plotFreq_1gram, cache=TRUE, dependson="ngrams"}
# Warningas.matrix is a massive memory hog!!!!!!!!!
# unigrams.frequency <- sort(colSums(as.matrix(unigrams)), decreasing = TRUE)
# unigrams.df <- data.frame(Word = names(unigrams.frequency), Frequency = unigrams.frequency)

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.unigram <- 1200
unigrams.subset <- unigrams[,match(findFreqTerms(unigrams, freqCutOff.unigram), 
                                   unigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
unigrams.freq <- sort(colSums(as.matrix(unigrams.subset)), decreasing = TRUE)
unigrams.df <- data.frame(Word = names(unigrams.freq), 
                          Frequency = unigrams.freq)

ggplot(data=unigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("1-grams with frequency greater than ", freqCutOff.unigram)) +
    coord_flip()
```



```{r plotFreq_2gram, echo=FALSE, cache=TRUE, dependson="ngrams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.bigram <- 75
bigrams.subset <- bigrams[,match(findFreqTerms(bigrams, freqCutOff.bigram), 
                                   bigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
bigrams.freq <- sort(colSums(as.matrix(bigrams.subset)), decreasing = TRUE)
bigrams.df <- data.frame(Word = names(bigrams.freq), 
                          Frequency = bigrams.freq)

ggplot(data=bigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 12),  
          axis.text.y = element_text(size = 10), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("2-grams with frequency greater than ", freqCutOff.bigram)) +
    coord_flip()
```

```{r plotFreq_3gram, echo=FALSE, cache=TRUE, dependson="ngrams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.trigram <- 100
trigrams.subset <- trigrams[,match(findFreqTerms(trigrams, freqCutOff.trigram), 
                                   trigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
trigrams.freq <- sort(colSums(as.matrix(trigrams.subset)), decreasing = TRUE)
trigrams.df <- data.frame(Word = names(trigrams.freq), 
                          Frequency = trigrams.freq)

ggplot(data=trigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("3-grams with frequency greater than ", freqCutOff.trigram)) +
    coord_flip()
```


# Next Steps

Using these n-grams, we'll be able to create a predictive model, wrapped in a Shiny app. The basic idea is that a user will type words into a text input box, and suggestions made by looking up probabilities derived from the frequencies estimated above. Ideally, the prediction would be dynamic, e.g. initiated by pressing space bar. 

Obviously, there's a lot more work to be done, such as (but not limited to):

* further cleaning the dataset
* generating n-grams from larger subsets (or merging results from multiple subsets)
* frequency cut-offs
* accuracy (linked to the above two items)
* how to deal with unrecognizable words
* stemming, and handling of typos
* optimizing for speed
* memory concerns


# Model

Ok, let's use the n-grams above.

```{r}
myText <- c("^case of ", 
            "^mean the ", 
            "^me the ", 
            "^but the ",
            "^at the ",
            "^on my ",
            "^quite some ", 
            "^his little ",
            "^during the ",
            "^must be ")[10]

# Remove leading and trailing spaces
# send through the same routine as for the corpora
# Find match in bi-grams and tri-grams. Estimate 

trigrams.test <- trigrams[,grep(myText,trigrams$dimnames$Terms)]
trigrams.freq <- sort(colSums(as.matrix(trigrams.test)), decreasing = TRUE)
trigrams.df <- data.frame(Word = names(trigrams.freq), 
                          Frequency = trigrams.freq)
head(trigrams.df)

```

