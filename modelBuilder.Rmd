---
title: "Data Science Capstone - Model Builder"
author: "Baldvin Einarsson"
date: "11/8/2017"
output:
  html_document:
    keep_md: yes
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(upload.fun = image_uri)
opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/cache/modelBuilder/")

library(ggplot2)
library(magrittr)
library(tokenizers)
library(tm)
library(RWeka)

# Holy smokes, it took forever to install rJava, see
# https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)

# library(ctv)
# install.views("NaturalLanguageProcessing")
```


# Load Data

The data comes from the following location:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), which we've downloaded.

Now, in order to load into `R`, let's make our lives easier and create an object called `fileNames.US` which contains the file names. It's value is the following:
```{r setFileNamesEtc, echo=FALSE}
# Set the directory ...
fileDir <- "./data/final/"
# ... and language file name conventions:
fileLanguage <- list(English = "en_US", Finnish = "fi_FI",
                     German = "de_DE", Russian = "ru_RU")

# This gets appended to the `fileDir.lang`
fileNameSuffix <- list(Blogs = '.blogs.txt',
                       News = '.news.txt',
                       Twitter = '.twitter.txt')
# Set up a list of file names:
fileNames.US <- 
    as.list(paste0(fileDir,fileLanguage$English, "/", 
                   fileLanguage$English,fileNameSuffix))
names(fileNames.US) <- names(fileNameSuffix)
paste(names(fileNames.US), unlist(fileNames.US), sep = ": ", collapse = "\n") %>% cat
```

We are now in a position to read in the files. Note that by typing `file` in a terminal, followed by the file name, the file encoding was listed as UTF-8.
```{r readData, message=FALSE, results='hide', cache=TRUE}
fileConnections <- lapply(X = fileNames.US, FUN = file)
corpus <- lapply(X = fileConnections, FUN = readLines, skipNul = TRUE, encoding = "UTF-8")
lapply(X = fileConnections, FUN = close)
```

# Exploratory Data Analysis

Let's do some basic inspection of the files and loaded objects. This is pretty basic, so we hide the code. Note that the word count comes from package `tokenizers`:
```{r basicFileAndObjectInfo, echo=FALSE, cache=TRUE, dependson="readData"}
kable(data.frame(Type = names(fileNames.US),
                 FileName = unlist(fileNames.US),
                 FileSize = (fileNames.US %>% 
                                 sapply(., FUN = file.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 ObjectSize = (corpus %>% 
                                 sapply(., FUN = object.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 LineCount = format(sapply(corpus, length),
                                    big.mark = ",", nsmall = 0, digits = 4),
                 WordCount =  sapply(X = corpus, 
                                     FUN = function(x){
                                         tokenize_words(x, lowercase = TRUE) %>%
                                             unlist %>%
                                             unique %>%
                                             length %>%
                                             format(big.mark = ",", digits = 4, nsmall = 0)
                                         })
                 ),
      row.names = FALSE,
      col.names = c("File type", "File name","File Size (MB)", "Object Size (MB)", 
                    "Line count", "Word (from 'tokenizers')"), 
      align = c("l","l", "r","r","r","r")
      )
```

We want to clean the data bit, such as

* change to lower case
* remove Internet links
* remove non-ascii characters
* remove double entries
* remove superfluous whitespace
* remove punctuation
* remove numbers
* remove profanity!

In order to do this, let's have a look at the [Natural Langue Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). There are plenty of packages to choose from, but for now let's start with [tm](https://cran.r-project.org/web/packages/tm/index.html).

In order to get things done faster, we pick out 2 % of entries from each type of data sources:
```{r corpusSubsetting, cache=TRUE, dependson="readData"}
set.seed(42)
useRatio <- 0.02
subsetCorpus <- 
    c(sample(corpus$Blogs, size = useRatio*length(corpus$Blogs), replace = FALSE),
      sample(corpus$News, size = useRatio*length(corpus$News), replace = FALSE),
      sample(corpus$Twitter, size = useRatio*length(corpus$Twitter), replace = FALSE)
    )
rm(corpus); invisible(gc())
```

Before we get started, let's download a list of profanity words commonly used. Here we take the list form [this website](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en). Enter at your on risk.


We then follow the instructions in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) and create an object which `tm` can work with, followed by a lot of cleaning. Most are steps are self-explanatory, but comments are provided:
```{r corpusCleaning, echo=TRUE, cache=TRUE, dependson=c("corpusSubsetting","readData")}
# Here, VCorpus stands for "Volatile Corpus"
# Also, using VectorSource, means that each vector entry is treated like a "document"
myCorpus <- VCorpus(VectorSource(subsetCorpus))
rm(subsetCorpus); invisible(gc())

# Function to remove patterns
toRemove <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
toRemove.empty <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toChange <- content_transformer(function(x, pattern, replacement) gsub(pattern, replacement, x))
toASCII <- content_transformer(function(x) iconv(x, "", "ASCII", "byte"))
# OK, so here we go:

# Set to ASCII:
myCorpus <- tm_map(myCorpus, toASCII)
# Remove troublesome non-ASCII:
myCorpus <- tm_map(myCorpus, toRemove.empty, "<.{2}>")
# Remove ftp or http links:
myCorpus <- tm_map(myCorpus, toRemove, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# Remove twitter handles (i.e. anything which has @ symbol followed by non-blank spaces:
myCorpus <- tm_map(myCorpus, toRemove, "@[^\\s]+")
# Set to lower case
myCorpus <- tm_map(myCorpus, tolower)
myCorpus.noStop <- tm_map(myCorpus, removeWords, stopwords("en"))
# Remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus.noStop <- tm_map(myCorpus.noStop, removePunctuation)
# Remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus.noStop <- tm_map(myCorpus.noStop, removeNumbers)
# Strip of superfluous whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus.noStop <- tm_map(myCorpus.noStop, stripWhitespace)
# Set to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus.noStop <- tm_map(myCorpus.noStop, PlainTextDocument)
```

Note that we removed the so called "stop words", but saved into a new object called `myCorpus.noStop`. This object we will save for the trigrams and higher n-grams, because phrases of that length probably contain a stop word. Let's inspect which words we just removed:
```{r showStopWords}
stopwords("en")
```

```{r echo=FALSE}
invisible(gc())
```
Note that we have not done any word stemming so far, which might reduce memory usage. However, this is something I'll have to investigate further.

Again, we follow the examples in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf), and tokenize the data, and create n-grams for values 1, 2, and 3. 

```{r ngrams, cache=TRUE, dependson=c("corpusCleaning","corpusSubsetting", "readData")}
unigramTokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
BigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
TrigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
QuadgramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 4, max = 4))
}

unigrams <- 
    DocumentTermMatrix(myCorpus.noStop, 
                       control = list(tokenize = unigramTokenizer))
bigrams <- DocumentTermMatrix(myCorpus.noStop, control = list(tokenize = BigramTokenizer))
trigrams <- DocumentTermMatrix(myCorpus, control = list(tokenize = TrigramTokenizer))
quadgrams <- DocumentTermMatrix(myCorpus, control = list(tokenize = QuadgramTokenizer))
```
```{r echo=FALSE}
rm(myCorpus); rm(myCorpus.noStop)
invisible(gc())
```

Then, we look at he n-grams with the highest frequency (found manually), an plot histograms. Again, we show the code for 1-grams, but hide the rest, which is nearly identical:

```{r}
getUnigramsFreq <- function(termNum) {
    sum(unigrams[,termNum])
}

tmp <- data.frame(Term = unigrams$dimnames$Terms,
                  Frequency = sapply(X = 1:dim(unigrams)[2], 
                                     FUN = getUnigramsFreq, 
                                     simplify = TRUE, 
                                     USE.NAMES = FALSE)
                  )

tmp <- tmp[order(tmp$Frequency, decreasing = TRUE), ]
row.names(tmp) <- 1:nrow(tmp)

```


```{r plotFreq_1gram, cache=TRUE, dependson="ngrams"}
# Warningas.matrix is a massive memory hog!!!!!!!!!
# unigrams.frequency <- sort(colSums(as.matrix(unigrams)), decreasing = TRUE)
# unigrams.df <- data.frame(Word = names(unigrams.frequency), Frequency = unigrams.frequency)

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.unigram <- 2400
unigrams.subset <- unigrams[,match(findFreqTerms(unigrams, freqCutOff.unigram), 
                                   unigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
unigrams.freq <- sort(colSums(as.matrix(unigrams.subset)), decreasing = TRUE)
unigrams.df <- data.frame(Word = names(unigrams.freq), 
                          Frequency = unigrams.freq)

ggplot(data=unigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("1-grams with frequency greater than ", freqCutOff.unigram)) +
    coord_flip()
```



```{r plotFreq_2gram, echo=FALSE, cache=TRUE, dependson="ngrams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.bigram <- 150
bigrams.subset <- bigrams[,match(findFreqTerms(bigrams, freqCutOff.bigram), 
                                   bigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
bigrams.freq <- sort(colSums(as.matrix(bigrams.subset)), decreasing = TRUE)
bigrams.df <- data.frame(Word = names(bigrams.freq), 
                          Frequency = bigrams.freq)

ggplot(data=bigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 12),  
          axis.text.y = element_text(size = 10), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("2-grams with frequency greater than ", freqCutOff.bigram)) +
    coord_flip()
```

```{r plotFreq_3gram, echo=FALSE, cache=TRUE, dependson="ngrams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.trigram <- 200
trigrams.subset <- trigrams[,match(findFreqTerms(trigrams, freqCutOff.trigram), 
                                   trigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
trigrams.freq <- sort(colSums(as.matrix(trigrams.subset)), decreasing = TRUE)
trigrams.df <- data.frame(Word = names(trigrams.freq), 
                          Frequency = trigrams.freq)

ggplot(data=trigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("3-grams with frequency greater than ", freqCutOff.trigram)) +
    coord_flip()
```


# Next Steps

Using these n-grams, we'll be able to create a predictive model, wrapped in a Shiny app. The basic idea is that a user will type words into a text input box, and suggestions made by looking up probabilities derived from the frequencies estimated above. Ideally, the prediction would be dynamic, e.g. initiated by pressing space bar. 

Obviously, there's a lot more work to be done, such as (but not limited to):

* further cleaning the dataset
* generating n-grams from larger subsets (or merging results from multiple subsets)
* frequency cut-offs
* accuracy (linked to the above two items)
* how to deal with unrecognizable words
* stemming, and handling of typos
* optimizing for speed
* memory concerns


# Model

Ok, let's use the n-grams above.

```{r}
myText <- c("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 
            "You're the reason why I smile everyday. Can you follow me please? It would mean the", 
            "Hey sunshine, can you follow me and make me the", 
            "Very early observations on the Bills game: Offense still struggling but the",
            "Go on a romantic date at the",
            "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my",
            "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some", 
            "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little",
            "Be grateful for the good times and keep the faith during the",
            "If this isn't the cutest thing you've ever seen, then you must be")

potentialAnswers <- 
    list(c("soda","beer","pretzels","cheese"), # B: soda X, pretzels X, 
         c("most","universe","world","best"), # B: world
         c("happiest","bluest","smelliest","saddest"), # happiest
         c("defense","referees","players","crowd"), # defense
         c("movies","beach","grocery","mall"), # movies X, beach
         c("way","phone","horse","motorcycle"), # B: way
         c("weeks","thing","years","time"), # B: time
         c("fingers","ears","eyes","toes"), # B: fingers
         c("hard","bad","sad","worse"), # bad
         c("asleep","insensitive","callous","insane")) # callous X, 

# Remove leading and trailing spaces
# send through the same routine as for the corpora
# Find match in bi-grams and tri-grams. Estimate 

trigrams.test <- trigrams[,grep("^case of",trigrams$dimnames$Terms)]
trigrams.freq <- sort(colSums(as.matrix(trigrams.test)), decreasing = TRUE)
trigrams.df <- data.frame(Word = names(trigrams.freq), 
                          Frequency = trigrams.freq)
head(trigrams.df)

```

