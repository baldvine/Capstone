---
title: "Data Science Capstone - Model Builder"
author: "Baldvin Einarsson"
date: "11/8/2017"
output:
  html_document:
    keep_md: yes
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(upload.fun = image_uri)
opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/cache/modelBuilder/")

library(ggplot2)
library(magrittr)
library(data.table)
library(tokenizers)
library(tm)
library(RWeka)

# Holy smokes, it took forever to install rJava, see
# https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)

# library(ctv)
# install.views("NaturalLanguageProcessing")
```


# Load Data

The data comes from the following location:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), which we've downloaded.

Now, in order to load into `R`, let's make our lives easier and create an object called `fileNames.US` which contains the file names. It's value is the following:
```{r setFileNamesEtc, echo=FALSE}
# Set the directory ...
fileDir <- "./data/final/"
# ... and language file name conventions:
fileLanguage <- list(English = "en_US", Finnish = "fi_FI",
                     German = "de_DE", Russian = "ru_RU")

# This gets appended to the `fileDir.lang`
fileNameSuffix <- list(Blogs = '.blogs.txt',
                       News = '.news.txt',
                       Twitter = '.twitter.txt')
# Set up a list of file names:
fileNames.US <- 
    as.list(paste0(fileDir,fileLanguage$English, "/", 
                   fileLanguage$English,fileNameSuffix))
names(fileNames.US) <- names(fileNameSuffix)
paste(names(fileNames.US), unlist(fileNames.US), sep = ": ", collapse = "\n") %>% cat
```

We are now in a position to read in the files. Note that by typing `file` in a terminal, followed by the file name, the file encoding was listed as UTF-8.
```{r readData, message=FALSE, results='hide', cache=TRUE}
fileConnections <- lapply(X = fileNames.US, FUN = file)
corpus <- lapply(X = fileConnections, FUN = readLines, skipNul = TRUE, encoding = "UTF-8")
lapply(X = fileConnections, FUN = close)
```

Let's count the number of elements in each:
```{r}
numElements <- sapply(corpus, length)
print(numElements)
```


# Data Scrubbing

We want to clean the data bit, such as

* change to lower case
* remove Internet links
* remove non-ascii characters
* remove double entries
* remove superfluous whitespace
* remove punctuation
* remove numbers
* remove profanity!

In order to do this, let's have a look at the [Natural Langue Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). There are plenty of packages to choose from, but for now let's start with [tm](https://cran.r-project.org/web/packages/tm/index.html).
```{r getBadWords}
library(RCurl)
options(RCurlOptions = list(ssl.verifypeer = FALSE))

badWords <- 
    read.csv(text = 
                 getURL("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"), stringsAsFactors = FALSE)[,1]
# Last line is an emoji:
badWords <- badWords[-length(badWords)]
# Ensure that no extra whitespace exitsts at either end:
badWords <- trimws(badWords, which = "both")
# Also ensure that no extra space:
badWords <- gsub(" {2,}", "", badWords)
# Count the number of words:
countBadWords <- 
    badWords %>% 
    strsplit(split = " ") %>% 
    sapply(FUN = length)

```



```{r corpusSubsetting, cache=TRUE, dependson="readData"}
set.seed(42)
useRatio <- 0.25

lapply(corpus, length)

subsetCorpus <- 
    c(sample(corpus$Blogs, size = useRatio*length(corpus$Blogs), replace = FALSE),
      sample(corpus$News, size = useRatio*length(corpus$News), replace = FALSE),
      sample(corpus$Twitter, size = useRatio*length(corpus$Twitter), replace = FALSE)
    )
rm(corpus); invisible(gc())
```

Before we get started, let's download a list of profanity words commonly used. Here we take the list form [this website](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en). Enter at your on risk.
```{r getBadWords}
library(RCurl)
options(RCurlOptions = list(ssl.verifypeer = FALSE))

badWords <- 
    getURL("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en") %>% 
    read.csv(text = .,
             stringsAsFactors = FALSE,
             header = FALSE) %>% 
    '['(,1)

# Take out two or more spaces:
badWords <- gsub(" {2,}","",badWords)
# Take out padding and superfluous whitespace:
badWords <- trimws(badWords, which = "both")
badWords <- badWords[-length(badWords)] 
```


We then follow the instructions in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) and create an object which `tm` can work with, followed by a lot of cleaning. Most are steps are self-explanatory, but comments are provided:
```{r corpusCleaning, echo=TRUE, cache=TRUE, dependson=c("corpusSubsetting","readData")}
# Here, VCorpus stands for "Volatile Corpus"
# Also, using VectorSource, means that each vector entry is treated like a "document"
myCorpus <- VCorpus(VectorSource(subsetCorpus))
rm(subsetCorpus); invisible(gc())

# Function to remove patterns
toRemove.space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
toRemove.empty <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toChange <- content_transformer(function(x, pattern, replacement) gsub(pattern, replacement, x))
toASCII <- content_transformer(function(x) iconv(x, "", "ASCII", "byte"))

# OK, so here we go:

# Set to ASCII:
myCorpus <- tm_map(myCorpus, toASCII)
# Remove troublesome non-ASCII:
myCorpus <- tm_map(myCorpus, toRemove.empty, "<.{2}>")
# Remove ftp or http links:
myCorpus <- tm_map(myCorpus, toRemove.space, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# Remove twitter stuff (i.e. anything which has @ or # followed by non-blank spaces:
myCorpus <- tm_map(myCorpus, toRemove.space, "@[^ ]+|#[^ ]+")
# Set to lower case
myCorpus <- tm_map(myCorpus, tolower)
myCorpus.noStop <- tm_map(myCorpus, removeWords, stopwords("en"))
# Remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus.noStop <- tm_map(myCorpus.noStop, removePunctuation)
# Remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus.noStop <- tm_map(myCorpus.noStop, removeNumbers)
# Strip of superfluous whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus.noStop <- tm_map(myCorpus.noStop, stripWhitespace)
# Set to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus.noStop <- tm_map(myCorpus.noStop, PlainTextDocument)
```

Note that we removed the so called "stop words", but saved into a new object called `myCorpus.noStop`. This object we will save for the trigrams and higher n-grams, because phrases of that length probably contain a stop word. Let's inspect which words we just removed:
```{r showStopWords}
stopwords("en")
```


Note that we have not done any word stemming so far, which might reduce memory usage. However, this is something I'll have to investigate further.

Again, we follow the examples in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf), and tokenize the data, and create n-grams for values 1, 2, and 3. 

```{r ngrams, cache=TRUE, dependson=c("corpusCleaning","corpusSubsetting", "readData")}
unigramTokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
BigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
TrigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
QuadgramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 4, max = 4))
}

unigrams.dtm <- 
    DocumentTermMatrix(myCorpus.noStop, 
                       control = list(tokenize = unigramTokenizer))
bigrams.dtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
bigrams.noStop.dtm <- DocumentTermMatrix(myCorpus.noStop, control = list(tokenize = BigramTokenizer))
trigrams.dtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = TrigramTokenizer))
quadgrams.dtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = QuadgramTokenizer))
```
```{r echo=FALSE}
rm(myCorpus); rm(myCorpus.noStop)
invisible(gc())
```


```{r include=FALSE}
# These are some of my improvisations

# Way to slow and memory intensive:
# getUnigramsFreq <- function(termNum) {
#     sum(unigrams[,termNum])
# }
# tmp <- data.frame(Term = unigrams$dimnames$Terms[1:1000],
#                   Frequency = sapply(X = 1:1000,#dim(unigrams)[2], 
#                                      FUN = getUnigramsFreq, 
#                                      simplify = TRUE, 
#                                      USE.NAMES = FALSE)
#                   )

# Works:
# tmp <- data.table::data.table(docIndex = unigrams$i,
#                               termIndex = unigrams$j,
#                               termFrequencyInDoc = unigrams$v)
# tmp[, Frequency := sum(termFrequencyInDoc), 
#     by = termIndex][order(Frequency, decreasing = TRUE)]
# tmp[, .(termFrequency = sum(termFrequencyInDoc)), 
#     by = termIndex][order(termFrequency, decreasing = TRUE)]

# Even better:
# tmp <- data.table::data.table(term = unigrams$dimnames$Terms[unigrams$j],
#                               termFrequencyInDoc = unigrams$v)
# tmp[, Frequency := sum(termFrequencyInDoc), 
#     by = term][order(Frequency, decreasing = TRUE)]
# tmp[, .(termFrequency = sum(termFrequencyInDoc)), 
#     by = term][order(termFrequency, decreasing = TRUE)]
```

Now, let's extract the unigrams into a little more user friendly format:

```{r createDTs, cache=TRUE, dependson=c("ngrams","corpusCleaning","corpusSubsetting", "readData")}

unigrams <- 
    data.table(Term = unigrams.dtm$dimnames$Terms[unigrams.dtm$j],
               termFrequencyInDoc = unigrams.dtm$v)[, .(Frequency = sum(termFrequencyInDoc)), 
                                                by = Term][order(Frequency, decreasing = TRUE)]

bigrams <- 
    data.table(Term = bigrams.dtm$dimnames$Terms[bigrams.dtm$j],
               termFrequencyInDoc = bigrams.dtm$v)[, .(Frequency = sum(termFrequencyInDoc)), 
                                                by = Term][order(Frequency, decreasing = TRUE)]
bigrams.noStop <- 
    data.table(Term = bigrams.noStop.dtm$dimnames$Terms[bigrams.noStop.dtm$j],
               termFrequencyInDoc = bigrams.noStop.dtm$v)[, .(Frequency = sum(termFrequencyInDoc)), 
                                                      by = Term][order(Frequency, decreasing = TRUE)]

trigrams <- 
    data.table(Term = trigrams.dtm$dimnames$Terms[trigrams.dtm$j],
               termFrequencyInDoc = trigrams.dtm$v)[, .(Frequency = sum(termFrequencyInDoc)), 
                                                by = Term][order(Frequency, decreasing = TRUE)]

quadgrams <- 
    data.table(Term = quadgrams.dtm$dimnames$Terms[quadgrams.dtm$j],
               termFrequencyInDoc = quadgrams.dtm$v)[, .(Frequency = sum(termFrequencyInDoc)), 
                                                by = Term][order(Frequency, decreasing = TRUE)]

```
```{r rmNGrams.dtm}
rm(list = c("unigrams.dtm", "bigrams.dtm", "bigrams.noStop.dtm", "trigrams.dtm", "quadgrams.dtm"))
invisible(gc())
```

Ok, let's remove the profanity:

```{r removeBadWords}
profanityRegExp <- paste0("(^| )",badWords,"( |$)", collapse = "|")
# grep(profanityRegExp, Term, invert = TRUE)
unigrams <- unigrams[grep(profanityRegExp, Term, invert = TRUE)]
bigrams <- bigrams[grep(profanityRegExp, Term, invert = TRUE)]
bigrams.noStop <- bigrams.noStop[grep(profanityRegExp, Term, invert = TRUE)]
trigrams <- trigrams[grep(profanityRegExp, Term, invert = TRUE)]
quadgrams <- quadgrams[grep(profanityRegExp, Term, invert = TRUE)]
```


```{r saveNGrams}
save(list = c("unigrams", 
              "bigrams", "bigrams.noStop", 
              "trigrams", 
              "quadgrams"), 
     file = "./data/ngrams_quarter.RData")
```



Ok, let's inspect the 1-grams and see how many words are needed to capture a certain percentage of words frequencies:

```{r}
Proportion <- cumsum(unigrams$Frequency)/sum(unigrams$Frequency)
numWords <- length(Proportion)
plot(Proportion)

which.max(Proportion > 0.975)/numWords

percentiles <- seq(0.90,0.99,by = 0.01)
100*sapply(X = percentiles, FUN = function(x) which.max(Proportion > x)/numWords)

#which.max(Proportion > 0.95)/numWords

```


Then, we look at he top 25 or so n-grams, and plot histograms. Again, we show the code for 1-grams, but hide the rest, which is nearly identical:



```{r plotFreq_1gram, cache=TRUE, dependson=c("createDFs","ngrams","corpusCleaning","corpusSubsetting", "readData")}

ggplot(data = head(unigrams, n = 25), 
       aes(x = reorder(Term, Frequency), y = Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle("Top 30 unigrams in frequency") +
    coord_flip()

```




```{r plotFreq_2gram, echo=FALSE, cache=TRUE, dependson=c("createDFs","ngrams","corpusCleaning","corpusSubsetting", "readData")}

ggplot(data = head(bigrams, n = 25), 
       aes(x = reorder(Term, Frequency), y = Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle("Top 25 bigrams in frequency") +
    coord_flip()

```

```{r plotFreq_2gram, echo=FALSE, cache=TRUE, dependson=c("createDFs","ngrams","corpusCleaning","corpusSubsetting", "readData")}

ggplot(data = head(bigrams.noStop, n = 25), 
       aes(x = reorder(Term, Frequency), y = Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle("Top 25 bigrams (no stop words) in frequency") +
    coord_flip()

```

```{r plotFreq_3gram, cache=TRUE, dependson=c("createDFs","ngrams","corpusCleaning","corpusSubsetting", "readData")}

ggplot(data = head(trigrams, n = 25), 
       aes(x = reorder(Term, Frequency), y = Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle("Top 30 trigrams in frequency") +
    coord_flip()

```


```{r plotFreq_4gram, cache=TRUE, dependson=c("createDFs","ngrams","corpusCleaning","corpusSubsetting", "readData")}

ggplot(data = head(quadgrams, n = 30), 
       aes(x = reorder(Term, Frequency), y = Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle("Top 30 quadgrams in frequency") +
    coord_flip()

```

# Next Steps

Using these n-grams, we'll be able to create a predictive model, wrapped in a Shiny app. The basic idea is that a user will type words into a text input box, and suggestions made by looking up probabilities derived from the frequencies estimated above. Ideally, the prediction would be dynamic, e.g. initiated by pressing space bar. 

Obviously, there's a lot more work to be done, such as (but not limited to):

* further cleaning the dataset
* generating n-grams from larger subsets (or merging results from multiple subsets)
* frequency cut-offs
* accuracy (linked to the above two items)
* how to deal with unrecognizable words
* stemming, and handling of typos
* optimizing for speed
* memory concerns


# Model

Ok, let's use the n-grams above.

```{r inlude=FALSE}
myText <- "this is spinal"

# Remove leading and trailing spaces
# send through the same routine as for the corpora
# Find match in bi-grams and tri-grams. Estimate 

prepareWords <- function(someText) {
    # Lower case
    someText <- tolower(someText)
    # Remove ftp or http links:
    someText <- gsub("(f|ht)tp(s?)://(.*)[.][a-z]+","",someText)
    # Remove twitter handles (i.e. anything which has @ symbol followed by non-blank spaces:
    gsub("@[^\\s]+","",someText)
    # Remove numbers:
    someText <- gsub("[0-9]+","",someText)
    # Remove punctuation:
    someText <- gsub("[',\\.!,?]","",someText)
    # Remove whitespace:
    someText <- trimws(someText, "both")
    someText <- gsub(" +", " ", someText)
    return(someText)
}


# Assumes that a single space separates words 
getLastNWords <- function(wordString, N = 1){
    # Convert to class "character":
    wordString <- as.character(wordString)
    # Extract a vector of the words
    wordSample <- unlist(strsplit(wordString, " "))
    # Handle length issues: Return vector of length N filled with NA's
    wordSample <- wordSample[(length(wordSample) - N + 1):length(wordSample)]
    return(paste(wordSample))
}

text4ngram <- function(someText, n = 1) {
    paste0("^",
           paste0(getLastNWords(prepareWords(someText),n),collapse = " ")
           )
} 


quadgrams[grep(text4ngram(myText,3),Term)] %>% print
trigrams[grep(text4ngram(myText,2),Term)]
bigrams[grep(text4ngram(myText),Term)]

trigrams[grep("^monkeys this",Term)]


```


