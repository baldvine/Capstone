---
title: "Data Science Capstone Milestone Report"
author: "Baldvin Einarsson"
date: "10/30/2017"
output:
  html_document:
    keep_md: yes
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/cache/")
library(ggplot2)
library(magrittr)
library(knitr)
library(tokenizers)
library(tm)
library(RWeka)

# Holy smokes, it took forever to install rJava, see
# https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)

# library(ctv)
# install.views("NaturalLanguageProcessing")
```

This is a milestone report for the Data Scientist Specialization Capstone project from [Coursera](https://www.coursera.org). Below, we'll give code which downloads the data, loads it in, and performs basic exploratory data analysis. This will form the basis for a predictive model, which we intend to create a Shiny app for.

# Download and load data

The data comes from the following location:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). Let's download it (if necessary!):

```{r downloadData}

fileURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/"
fileZip <- "./data/Coursera-SwiftKey.zip"

if (!file.exists(fileZip)){
    cat("Downloading. This may take a while...")
    download.file(url=fileURL, destfile = fileZip, method='curl', quiet = TRUE)
}

```

Next, we do some (manual) inspecting of the files in the `.zip` file. We see (again, manually), that all the zipped files are stored in a folder called `final/`. Let's unzip the files if that directory does not exist:

```{r unzipFiles}
# Get the file names of the files in the zip:
zippedFiles <- unzip(zipfile = fileZip, list=TRUE)

# Obtain the directory name:
zipDir <- strsplit(zippedFiles$Name[1],split="/")[[1]][1]

if (!dir.exists(paste0("./data/",zipDir))) {
    unzip(zipfile = fileZip, exdir = "./data")
}
```

Now, in order to load into `R`, let's make our lives easier and create an object called `fileNames.US` which contains the file names. It's value is the following:
```{r setFileNamesEtc, echo=FALSE}
# Set the directory ...
fileDir <- "./data/final/"
# ... and language file name conventions:
fileLanguage <- list(English = "en_US", Finnish = "fi_FI",
                     German = "de_DE", Russian = "ru_RU")

# This gets appended to the `fileDir.lang`
fileNameSuffix <- list(Blogs = '.blogs.txt',
                       News = '.news.txt',
                       Twitter = '.twitter.txt')
# Set up a list of file names:
fileNames.US <- 
    as.list(paste0(fileDir,fileLanguage$English, "/", 
                   fileLanguage$English,fileNameSuffix))
names(fileNames.US) <- names(fileNameSuffix)
paste(names(fileNames.US), unlist(fileNames.US), sep = ": ", collapse = "\n") %>% cat
```

We are now in a position to read in the files:
```{r readData, message=FALSE, results='hide', cache=TRUE}
fileConnections <- lapply(X = fileNames.US, FUN = file)
corpus <- lapply(X = fileConnections, FUN = readLines, skipNul = TRUE, encoding = "Latin-1")
lapply(X = fileConnections, FUN = close)
```

# Exploratory Data Analysis

Let's do some basic inspection of the files and loaded objects. Note that the word count comes from package `tokenizers`:
```{r basicFileAndObjectInfo, echo=FALSE, cache=TRUE, dependson="readData"}
kable(data.frame(Type = names(fileNames.US),
                 FileName = unlist(fileNames.US),
                 FileSize = (fileNames.US %>% 
                                 sapply(., FUN = file.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 ObjectSize = (corpus %>% 
                                 sapply(., FUN = object.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 LineCount = format(sapply(corpus, length),
                                    big.mark = ",", nsmall = 0, digits = 4),
                 WordCount =  sapply(X = corpus, 
                                     FUN = function(x){
                                         tokenize_words(x, lowercase = TRUE) %>%
                                             unlist %>%
                                             unique %>%
                                             length %>%
                                             format(big.mark = ",", digits = 4, nsmall = 0)
                                         })
                 ),
      row.names = FALSE,
      col.names = c("File type", "File name","File Size (MB)", "Object Size (MB)", 
                    "Line count", "Word (from 'tokenizers')"), 
      align = c("l","l", "r","r","r","r")
      )
```

We want to clean the data bit, such as

* change to lower case
* remove Internet links
* remove double entries
* remove superfluous whitespace
* remove punctuation
* remove numbers

In order to do this, let's have a look at the [Natural Langue Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). There are plenty of packages to choose from, but for now let's start with [tm](https://cran.r-project.org/web/packages/tm/index.html).

In order to get things done faster, we pick out 1 % of entries from each type of data sources:
```{r corpusSubsetting, cache=TRUE, dependson="readData"}
set.seed(42)
useRatio <- 0.01
subsetCorpus <- 
    c(sample(corpus$Blogs, size = useRatio*length(corpus$Blogs), replace = FALSE),
      sample(corpus$News, size = useRatio*length(corpus$News), replace = FALSE),
      sample(corpus$Twitter, size = useRatio*length(corpus$Twitter), replace = FALSE)
    )
rm(corpus); invisible(gc())
```


We then follow the instructions in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) and create an object which `tm` can work with. Ok, let's do a lot of cleaning. Most are self-explanatory, but comments are provided:
```{r echo=TRUE, cache=TRUE, dependson="corpusSubsetting"}
# Here, VCorpus stands for "Volatile Corpus"
# Also, using VectorSource, means that each vector entry is treated like a "document"
subsetCorpus <- VCorpus(VectorSource(subsetCorpus))

# Function to remove patterns
toRemove <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

# OK, so here we go:

# Remove ftp or http links:
subsetCorpus <- tm_map(subsetCorpus, toRemove, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# Remove twitter handles (i.e. anything which has @ symbol followed by non-blank spaces:
subsetCorpus <- tm_map(subsetCorpus, toRemove, "@[^\\s]+")
# Set to lower case
subsetCorpus <- tm_map(subsetCorpus, tolower)
# Remove so called "stop words". Type stopwords("en") to see which they are
subsetCorpus <- tm_map(subsetCorpus, removeWords, stopwords("en"))
# Remove punctuation
subsetCorpus <- tm_map(subsetCorpus, removePunctuation)
# Remove numbers
subsetCorpus <- tm_map(subsetCorpus, removeNumbers)
# Strip of superfluous whitespace
subsetCorpus <- tm_map(subsetCorpus, stripWhitespace)
# 
subsetCorpus <- tm_map(subsetCorpus, PlainTextDocument)
```


```{r ngrams}
unigramTokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
unigrams <- 
    DocumentTermMatrix(subsetCorpus, 
                       control = list(tokenize = unigramTokenizer))

# BigramTokenizer <- function(x) {
#         NGramTokenizer(x, Weka_control(min = 2, max = 2))
# }
# bigrams <- DocumentTermMatrix(subsetCorpus, control = list(tokenize = BigramTokenizer))

# TrigramTokenizer <- function(x) {
#         NGramTokenizer(x, Weka_control(min = 3, max = 3))
# }
# trigrams <- DocumentTermMatrix(us_files, control = list(tokenize = TrigramTokenizer))
```



```{r plotFreq}

# as.matrix is a massive memory hog!!!!!!!!!
# unigrams_frequency <- sort(colSums(as.matrix(unigrams)), decreasing = TRUE)
# unigrams_freq_df <- data.frame(Word = names(unigrams_frequency), Frequency = unigrams_frequency)

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
unigrams.subset <- unigrams[,match(findFreqTerms(unigrams, 1000), 
                                   unigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
unigrams.freq <- sort(colSums(as.matrix(unigrams.subset)), decreasing = TRUE)
unigrams.df <- data.frame(Word = names(unigrams.freq), 
                          Frequency = unigrams.freq)

ggplot(data=unigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10),  
          axis.text.y = element_text(size = 12)
          ) + 
    xlab("Word") +
    coord_flip()
```

