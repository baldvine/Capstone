---
title: "Data Science Capstone Milestone Report"
author: "Baldvin Einarsson"
date: "10/31/2017"
output:
  html_document:
    keep_md: yes
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
opts_knit$set(upload.fun = image_uri)
opts_chunk$set(echo = TRUE, comment = "", cache.path = "./data/cache/")

library(ggplot2)
library(magrittr)
library(tokenizers)
library(tm)
library(RWeka)

# Holy smokes, it took forever to install rJava, see
# https://github.com/MTFA/CohortEx/wiki/Run-rJava-with-RStudio-under-OSX-10.10,-10.11-(El-Capitan)-or-10.12-(Sierra)

# library(ctv)
# install.views("NaturalLanguageProcessing")
```

This is a milestone report for the Data Scientist Specialization Capstone project from [Coursera](https://www.coursera.org). Below, we'll give code which downloads the data, loads it in, and performs basic exploratory data analysis. This will form the basis for a predictive model for text, which we intend to create a Shiny app for.

Note that the markdown file is available on [GitHub](https://github.com/baldvine/Capstone/blob/master/MilestoneReport_BE.Rmd).

# Download and load data

The data comes from the following location:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). Let's download it (if necessary!):

```{r downloadData}

fileURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/"
fileZip <- "./data/Coursera-SwiftKey.zip"

if (!file.exists(fileZip)){
    cat("Downloading. This may take a while...")
    download.file(url=fileURL, destfile = fileZip, method='curl', quiet = TRUE)
}

```

Next, we do some (manual) inspecting of the files in the `.zip` file. We see (again, manually), that all the zipped files are stored in a folder called `final/`. Let's unzip the files if that directory does not exist:

```{r unzipFiles}
# Get the file names of the files in the zip:
zippedFiles <- unzip(zipfile = fileZip, list=TRUE)

# Obtain the directory name:
zipDir <- strsplit(zippedFiles$Name[1],split="/")[[1]][1]

if (!dir.exists(paste0("./data/",zipDir))) {
    unzip(zipfile = fileZip, exdir = "./data")
}
```

Now, in order to load into `R`, let's make our lives easier and create an object called `fileNames.US` which contains the file names. It's value is the following:
```{r setFileNamesEtc, echo=FALSE}
# Set the directory ...
fileDir <- "./data/final/"
# ... and language file name conventions:
fileLanguage <- list(English = "en_US", Finnish = "fi_FI",
                     German = "de_DE", Russian = "ru_RU")

# This gets appended to the `fileDir.lang`
fileNameSuffix <- list(Blogs = '.blogs.txt',
                       News = '.news.txt',
                       Twitter = '.twitter.txt')
# Set up a list of file names:
fileNames.US <- 
    as.list(paste0(fileDir,fileLanguage$English, "/", 
                   fileLanguage$English,fileNameSuffix))
names(fileNames.US) <- names(fileNameSuffix)
paste(names(fileNames.US), unlist(fileNames.US), sep = ": ", collapse = "\n") %>% cat
```

We are now in a position to read in the files:
```{r readData, message=FALSE, results='hide', cache=TRUE}
fileConnections <- lapply(X = fileNames.US, FUN = file)
corpus <- lapply(X = fileConnections, FUN = readLines, skipNul = TRUE, encoding = "Latin-1")
lapply(X = fileConnections, FUN = close)
```

# Exploratory Data Analysis

Let's do some basic inspection of the files and loaded objects. This is pretty basic, so we hide the code. Note that the word count comes from package `tokenizers`:
```{r basicFileAndObjectInfo, echo=FALSE, cache=TRUE, dependson="readData"}
kable(data.frame(Type = names(fileNames.US),
                 FileName = unlist(fileNames.US),
                 FileSize = (fileNames.US %>% 
                                 sapply(., FUN = file.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 ObjectSize = (corpus %>% 
                                 sapply(., FUN = object.size) %>% 
                                 '/'(.,1024) %>% 
                                 format(big.mark = ",", digits = 4, nsmall = 0)),
                 LineCount = format(sapply(corpus, length),
                                    big.mark = ",", nsmall = 0, digits = 4),
                 WordCount =  sapply(X = corpus, 
                                     FUN = function(x){
                                         tokenize_words(x, lowercase = TRUE) %>%
                                             unlist %>%
                                             unique %>%
                                             length %>%
                                             format(big.mark = ",", digits = 4, nsmall = 0)
                                         })
                 ),
      row.names = FALSE,
      col.names = c("File type", "File name","File Size (MB)", "Object Size (MB)", 
                    "Line count", "Word (from 'tokenizers')"), 
      align = c("l","l", "r","r","r","r")
      )
```

We want to clean the data bit, such as

* change to lower case
* remove Internet links
* remove double entries
* remove superfluous whitespace
* remove punctuation
* remove numbers

In order to do this, let's have a look at the [Natural Langue Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html). There are plenty of packages to choose from, but for now let's start with [tm](https://cran.r-project.org/web/packages/tm/index.html).

In order to get things done faster, we pick out 1 % of entries from each type of data sources:
```{r corpusSubsetting, cache=TRUE, dependson="readData"}
set.seed(42)
useRatio <- 0.01
subsetCorpus <- 
    c(sample(corpus$Blogs, size = useRatio*length(corpus$Blogs), replace = FALSE),
      sample(corpus$News, size = useRatio*length(corpus$News), replace = FALSE),
      sample(corpus$Twitter, size = useRatio*length(corpus$Twitter), replace = FALSE)
    )
rm(corpus); invisible(gc())
```


We then follow the instructions in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) and create an object which `tm` can work with, followed by a lot of cleaning. Most are steps are self-explanatory, but comments are provided:
```{r corpusCleaning, echo=TRUE, cache=TRUE, dependson=c("corpusSubsetting","readData")}
# Here, VCorpus stands for "Volatile Corpus"
# Also, using VectorSource, means that each vector entry is treated like a "document"
subsetCorpus <- VCorpus(VectorSource(subsetCorpus))

# Function to remove patterns
toRemove <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

# OK, so here we go:

# Remove ftp or http links:
subsetCorpus <- tm_map(subsetCorpus, toRemove, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# Remove twitter handles (i.e. anything which has @ symbol followed by non-blank spaces:
subsetCorpus <- tm_map(subsetCorpus, toRemove, "@[^\\s]+")
# Set to lower case
subsetCorpus <- tm_map(subsetCorpus, tolower)
# Remove so called "stop words". Type stopwords("en") to see which they are
subsetCorpus <- tm_map(subsetCorpus, removeWords, stopwords("en"))
# Remove punctuation
subsetCorpus <- tm_map(subsetCorpus, removePunctuation)
# Remove numbers
subsetCorpus <- tm_map(subsetCorpus, removeNumbers)
# Strip of superfluous whitespace
subsetCorpus <- tm_map(subsetCorpus, stripWhitespace)
# Set to plain text
subsetCorpus <- tm_map(subsetCorpus, PlainTextDocument)
```
```{r echo=FALSE}
invisible(gc())
```
Note that we have not done any word stemming so far, which might reduce memory usage. However, this is something I'll have to investigate further.

Again, we follow the examples in the [tm package vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf), and tokenize the data, and create n-grams for values 1, 2, and 3. Below, we show the code for 1-grams, but hide the nearly identical code for 2-grams and 3-grams.

```{r ngrams, cache=TRUE, dependson=c("corpusCleaning","corpusSubsetting", "readData")}
unigramTokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 1))
}
unigrams <- 
    DocumentTermMatrix(subsetCorpus, 
                       control = list(tokenize = unigramTokenizer))
```

```{r 2_3grams, echo=FALSE, cache=TRUE, dependson=c("corpusCleaning","corpusSubsetting", "readData")}
BigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
bigrams <- DocumentTermMatrix(subsetCorpus, control = list(tokenize = BigramTokenizer))

TrigramTokenizer <- function(x) {
    NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
trigrams <- DocumentTermMatrix(subsetCorpus, control = list(tokenize = TrigramTokenizer))
```
```{r echo=FALSE}
invisible(gc())
```

Then, we look at he n-grams with the highest frequency (found manually), an plot histograms. Again, we show the code for 1-grams, but hide the rest, which is nearly identical:

```{r plotFreq_1gram, cache=TRUE, dependson="ngrams"}
# Warningas.matrix is a massive memory hog!!!!!!!!!
# unigrams.frequency <- sort(colSums(as.matrix(unigrams)), decreasing = TRUE)
# unigrams.df <- data.frame(Word = names(unigrams.frequency), Frequency = unigrams.frequency)

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.unigram <- 1000
unigrams.subset <- unigrams[,match(findFreqTerms(unigrams, freqCutOff.unigram), 
                                   unigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
unigrams.freq <- sort(colSums(as.matrix(unigrams.subset)), decreasing = TRUE)
unigrams.df <- data.frame(Word = names(unigrams.freq), 
                          Frequency = unigrams.freq)

ggplot(data=unigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("1-grams with frequency greater than ", freqCutOff.unigram)) +
    coord_flip()
```



```{r plotFreq_2gram, echo=FALSE, cache=TRUE, dependson="2_3grams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.bigram <- 75
bigrams.subset <- bigrams[,match(findFreqTerms(bigrams, freqCutOff.bigram), 
                                   bigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
bigrams.freq <- sort(colSums(as.matrix(bigrams.subset)), decreasing = TRUE)
bigrams.df <- data.frame(Word = names(bigrams.freq), 
                          Frequency = bigrams.freq)

ggplot(data=bigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 12),  
          axis.text.y = element_text(size = 10), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("2-grams with frequency greater than ", freqCutOff.bigram)) +
    coord_flip()
```

```{r plotFreq_3gram, echo=FALSE, cache=TRUE, dependson="2_3grams"}

# Let's use the 'findFreqTerms()' function to filter out the low frequency words:
freqCutOff.trigram <- 10
trigrams.subset <- trigrams[,match(findFreqTerms(trigrams, freqCutOff.trigram), 
                                   trigrams$dimnames$Terms)]
# Note that we simply found the column IDs of the those terms and subset.
# Then, we compute the frequencies:
trigrams.freq <- sort(colSums(as.matrix(trigrams.subset)), decreasing = TRUE)
trigrams.df <- data.frame(Word = names(trigrams.freq), 
                          Frequency = trigrams.freq)

ggplot(data=trigrams.df, aes(x = reorder(Word, Frequency), y=Frequency)) + 
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 10),  
          axis.text.y = element_text(size = 12), 
          plot.title = element_text(hjust = 0.5)
          ) + 
    xlab("Word") + ggtitle(paste0("3-grams with frequency greater than ", freqCutOff.trigram)) +
    coord_flip()
```


# Next Steps

Using these n-grams, we'll be able to create a predictive model, wrapped in a Shiny app. The basic idea is that a user will type words into a text input box, and suggestions made by looking up probabilities derived from the frequencies estimated above. Ideally, the prediction would be dynamic, e.g. initiated by pressing space bar. 

Obviously, there's a lot more work to be done, such as (but not limited to):

* further cleaning the dataset
* generating n-grams from larger subsets (or merging results from multiple subsets)
* frequency cut-offs
* accuracy (linked to the above two items)
* how to deal with unrecognizable words
* stemming, and handling of typos
* optimizing for speed
* memory concerns